---
title: "Textual analysis of NPR's climate coverage"
author: "by Austin Fast"
date: "April 6, 2023"
#output: html_document
output: 
  html_document:
    code_folding: show
    toc: true
    toc_depth: 3
    toc_float: true
    theme: cosmo
---

# Why hire a reporter with data skills?

Anecdotes infuse heart and soul into a story, but data analysis creates an empirical backbone. It leads stories beyond the "he said" or "she said" of a few people's experiences to hold up a story's wider assertions about the world.

Government agencies and other institutions frequently store data and records online in clunky formats or they decline to share machine-readable data sets with journalists. Scraping from online dashboards and APIs (application programming interfaces) is one way to quickly unlock this data and mine it for story ideas. For example, coding a custom scraper for NPR's climate articles and running a textual analysis took me about two hours.

Other use cases I've turned into stories: 

* Automatically downloading each county's COVID-19 vaccination counts daily from the <a href="https://covid.cdc.gov/covid-data-tracker/#vaccinations_vacc-people-booster-percent-pop5" target="_blank">CDC's API</a>
  + Story: <a href="https://www.npr.org/sections/health-shots/2021/04/20/988871651/lagging-vaccination-rates-among-rural-seniors-hint-at-brewing-rural-urban-divide" target="_blank">Lagging Vaccination Rates Among Rural Seniors Hint At Brewing Rural-Urban Divide</a>
* Using Archive.org's API to scrape counts of ICE detainees with COVID-19 from saved versions of the <a href="https://www.ice.gov/coronavirus" target="_blank">ICE website</a> when ICE declined to share records
  + Story: <a href="https://www.npr.org/2021/08/13/1027095025/detainees-say-ice-exposes-them-to--covid-19" target="_blank">COVID Cases Are Rising In ICE Facilities, Putting Detainees And The Public At Risk</a>
* Extracting charitable donations to anti-abortion and abortion-rights causes from the <a href="https://pp-990-xml.s3.us-east-1.amazonaws.com/202133159349100603_public.xml?response-content-disposition=inline&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA266MJEJYTM5WAG5Y%2F20230407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230407T040442Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=84b029fdf86c8d8d13df74d7c4c156c8fec5c8579da504bbb1c5e9d1b74a9c9e" target="_blank">Shell Oil Company's IRS 990 form</a>
  + Story: <a href="https://www.wwno.org/politics/2022-10-06/mississippis-latest-move-in-its-anti-abortion-agenda-a-tax-break-for-corporate-donations" target="_blank">Mississippi's latest move in its anti-abortion agenda? A tax break for corporate donations</a>
* Scraping hundreds of comments on a proposed change to a <a href="https://www.regulations.gov/document/BOP-2023-0001-0001/comment?postedDateFrom=2022-11-04&postedDateTo=2023-02-01" target="_blank">Bureau of Prisons regulation</a> from the Federal Register's API
  + Story: Forthcoming by Kroc Fellow Tilda Wilson

# Findings
The <a href="https://www.npr.org/sections/climate/archive" target="_blank">climate desk's archive on NPR.org</a> includes <b>nearly 300 pieces published since October 2022</b>. 

* Just four stories mention the phrase "human health" and <a href="https://www.npr.org/2023/03/12/1162933860/californias-pajaro-river-breached-overnight-friday-causing-massive-flooding" target="_blank">only two</A> mention "<a href="https://www.npr.org/2023/01/07/1147678843/bay-area-residents-are-preparing-for-the-russian-river-to-flood-again-early-next
" target="_blank">mental health</A>," emphasizing the need for a reporter to cover how climate change is affecting our health. 
* Native Americans are among several disadvantaged communities facing outsized effects of climate change. I only found <a href="https://www.npr.org/2022/12/07/1140861347/un-biodiversity-convention-aims-to-slow-humanitys-war-with-nature-heres-whats-at-" target="_blank">two stories</a> mentioning <a href="https://www.npr.org/2022/12/06/1141047752/delegates-meet-with-a-mandate-to-set-global-biodiversity-goals-for-the-next-ten-" target="_blank">their displacement</a> and a small number of stories overall, suggesting NPR is undercovering how climate change affects their health.
* People living in poverty are less-equipped to mitigate the effects of climate change. NPR's coverage mentions the phrase "low-income" in 18 stories, largely due to a proposal to <a href="https://www.npr.org/2022/11/16/1136817427/low-income-countries-want-more-money-for-climate-damage-theyre-unlikely-to-get-i" target="_blank">boost clean energy</A> in <a href="https://www.npr.org/2022/11/09/1135546822/u-s-plan-for-boosting-climate-investment-in-low-income-countries-draws-criticism
" target="_blank">developing countries</a> at COP27 in November, but ongoing coverage of the connection between poverty and climate change has been far more scarce.
* Few articles mentioned [sewer overflows](#sewer) and [methane flaring](#flaring), suggesting space to follow up on the pitches I described in my cover letter.

![](outputs/Word pairs-over time.png)

***

![](outputs/Word pairs-health.png)

***

![](outputs/Word pairs-native.png)

***

![](outputs/Word pairs-poverty.png)

***

# Methodology
NPR disables automated access to its website through GET requests, so I spun up a Selenium server to build a list of story headlines and links from <b><a href="https://www.npr.org/sections/climate/archive" target="_blank">NPR's Climate archive</a></b> going back to Oct. 1, 2022.

![](npr-climate2.png)

Load R libraries
```{r message = FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(janitor)
library(lubridate)
library(data.table)
library(tidytext)
library(rvest)
library(httr)
library(wdman)
```

The archive website doesn't load all links until a user manually scrolls down on the page. This code uses a Selenium browser to scroll until September's stories start to appear. It then scrapes all the headlines, dates and URLs.
```{r eval=FALSE}
selServ <- selenium(
  port = 4444L,
  version = 'latest',
  chromever = '103.0.5060.134'
)

remDr <- remoteDriver(
  remoteServerAddr = 'localhost',
  port = 4444L,
  browserName = 'firefox'
)

Sys.sleep(5)

#Open browser
remDr$open()

#Go to NPR's climate archive, starting from today
climate <- paste0("https://www.npr.org/sections/climate/archive?date=", format(Sys.Date(), format="%m-%d-%Y"))
remDr$navigate(climate)
Sys.sleep(5)

#Grab HTML source code
source <- remDr$getPageSource()[[1]]

#Page is dynamically rendered as a user scrolls, so scroll down page until September's stories load
while (!str_detect(source, "datetime=\"2022-09-29\"")){
  webElem <- remDr$findElement("css", "body")

  webElem$sendKeysToElement(list(key = "down_arrow"))
  Sys.sleep(1)
  source <- remDr$getPageSource()[[1]]
}

#Read in the page source
page <- read_html(source)

#Pull links for all stories since October
links <- page %>%
  html_nodes ("h2") %>%
  html_nodes ("a") %>%
  html_attr('href') %>%
  as.data.frame() %>%
  magrittr::set_colnames("url")

#Pull headlines for all stories since October
headline <- page %>%
  html_nodes ("h2") %>%
  html_nodes ("a") %>%
  html_text() %>%
  as.data.frame() %>%
  magrittr::set_colnames("headline")

#Calculate date published from URL - 2 failed to parse because Short Wave episodes are named differently
date <- links %>%
  mutate (date = str_remove_all(url, "^https://www.npr.org/")) %>%
  mutate (date = str_remove_all(date, "^sections/goatsandsoda/")) %>%
  mutate (date = str_remove_all(date, "^sections/pictureshow/")) %>%
  mutate (date = str_remove_all(date, "^sections/health-shots/")) %>%
  mutate (date = ymd(substr(date,1,10)))

#Check for any dates that don't start with numbers? This should be zero.
date %>%
  filter (!str_detect(date, "[0-9]"))

#Create list of articles to scrape
df <- cbind (date, headline) %>%
  #Keep only those since Oct. 1, 2022
  filter (date > as.Date('2022-09-30'))
```

# Check links
```{r message=FALSE}
df <- read_csv("npr_scrape.csv") %>% select (2,3,1)

head (df)
```

# Scrape story text
This code takes the list of URLs and headlines created above and visits each URL in sequence to scrape the story text, byline and other details. It also remove unnecessary words, like reporter names, NPR jargon and descriptions of ambient sound that appear frequently but are not representative of the text corpus.

<div style="width:500px; height:333px">
![](npr-climate3.png)
</div>
```{r eval=FALSE}
output <- tibble()

for (i in 1:length(df$url)){
#for (i in 1:20){
  #i <- 9
  link <- df$url[i]
  #page <- html_session(link)
  #html <- content(page$response)

  remDr$navigate(link)
  Sys.sleep(4)
  
  source <- remDr$getPageSource()[[1]]
  html <- read_html(source)
  done=0
  
  #Pull body of story (DACS Transcript version)
  if (length(html %>%
    html_nodes(xpath="//*[@class='transcript storytext']") %>%
    html_text()) > 0){
    
      html2 <- html %>%
        html_nodes(xpath="//*[@class='transcript storytext']") %>%
        html_text() %>%
        #Remove speaker names and descriptions of ambient sound
        str_remove_all("(\\b[A-Z]+ *\\b[:]*){2,}") %>% #remove two words in all caps that may or may not have a space after them (names, ambi)
        str_remove_all ("[, ]*[A-Z]+:") %>% #remove words with a colon after them (, BYLINE:, HOST:, etc.)
        str_remove_all ("\\(\\)") %>%
        str_remove_all ("A MARTÍNEZ") %>%
        str_squish()
        
      #Delete content after copyright notice
      html2 <- html2 %>%
        str_split ("Copyright ©")
      
      #Keep only before copyright notice
      html2 <- html2[[1]][[1]]
      
      done = 1
  }
  
  #Pull article if it's not a DACS auto-transcript
  #if (length(html %>%
  #    html_nodes(xpath="//*[@class='story']") %>%
  #    html_text()) > 0){
  if (done==0){

      html2 <- html %>%
        html_nodes(xpath="//*[@id='storytext']") %>%
        html_nodes("p") %>%
        #Unsuccessfully tried to remove photo captions at top
        #html_nodes("p>:not(xpath='//*[@class='credit-caption']") %>%
        #html_attrs(">:not(xpath='//*[@class='credit-caption']')") %>%
        html_text() %>%
        #Remove speaker names and descriptions of ambient sound
        str_remove_all("(\\b[A-Z]+ *\\b[:]*){2,}") %>% #remove two words in all caps that may or may not have a space after them (names, ambi)
        str_remove_all ("[, ]*[A-Z]+:") %>% #remove words with a colon after them (, BYLINE:, HOST:, etc.)
        str_remove_all ("\\(\\)") %>%
        str_remove_all ("A MARTÍNEZ") %>%
        str_remove_all ("Enlarge this image") %>%
        as.character %>%
        str_squish() %>%
        toString 
        
    #Delete content after phrase "More Stories From"
      html2 <- html2 %>%
        str_split ("More Stories From")
      
      #Keep only before copyright notice
      html2 <- html2[[1]][[1]]
  
  }
  
  #Check that I've pulled an article and break for loop if it's missing
  check <- length(html2)
  if (check<1){
    stop ("ERROR")
  }
  
  #Pull byline
  byline <- html %>%
    html_nodes(xpath="//*[@id='storybyline']/div/div/div/p") %>%
    html_text() %>%
    str_squish() %>%
    toString()
  
  #Pull slug (climate, weather, national, etc.)
  slug <- html %>%
    html_node(xpath="//h3[@class='slug']") %>%
    html_text() %>%
    str_squish() %>%
    toString()
    
  #Save to dataframe
  this_row <- tibble(byline=byline, body=html2, slug=slug)
  output <- bind_rows (output, this_row)
  rm(this_row, byline, html2, html, source, done, slug, check)
}

#Join scrape results to headline, url and date
join <- cbind (df, output)

#Save file
write_csv (join, "npr_scrape.csv")
```

# Examine results
What desks were these stories assigned on NPR.org?
```{r message=FALSE}
join <- read_csv("npr_scrape.csv")

#Check how are these categorized?
join %>% 
  count (slug) %>%
  arrange (desc(n)) %>%
  print
```

Take a look at the first story to check it scraped properly. The all-caps names of reporters and hosts and ambient sound descriptions have been removed.
```{r}
join %>%
  filter (row_number()==1) %>%
  pull (slug) %>%
  paste ("SLUG:", .) %>%
  print 

join %>%
  filter (row_number()==1) %>%
  pull (headline) %>%
  paste ("HED:", .) %>%
  print 

join %>%
  filter (row_number()==1) %>%
  pull (url) %>%
  paste ("URL:", .) %>%
  print 

join %>%
  filter (row_number()==1) %>%
  pull (byline) %>%
  paste ("BYLINE:", .) %>%
  print 

join %>%
  filter (row_number()==1) %>%
  pull (body) %>%
  paste ("BODY:", .) %>%
  print 
```

# Analyze results
This code tokenizes the text corpus to find frequency of words and word pairs within 272 stories published from Oct. 1, 2022-April 5, 2023. It also removes stop words, which are so common that they don't provide insight for textual analysis (e.g., "a", "an", "the").

The resulting list displays how frequently almost 29,000 unique word pairs appeared in NPR's climate coverage over the last six months.
```{r}
join2 <- join %>%
  unnest_tokens(word, body, token = "words") 

#Create list of words and remove stop words
join_words <- join2 %>%
  anti_join(stop_words, by="word") %>%  #went from 236700 to 107524
  filter(!str_detect(word, "http")) %>%
  filter(!str_detect(word, "t.co")) #removed an additional few words

#Remove URLs
replace_reg <- "https?://[^\\s]+|&amp;|&lt;|&gt;|\bRT\\b"

#Create list of word pairs (236430 word pairs)
join_bigrams <- join %>% 
  mutate(text = str_replace_all(body, replace_reg, "")) %>%
  unnest_tokens(bigram, body, token = "ngrams", n = 2) #kept returning error until added the collapse=FALSE statement

#Remove pairs containing stop words
join_bigrams <- join_bigrams %>%
  separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
  anti_join(stop_words, by = c("first" = "word")) %>%
  anti_join(stop_words, by = c("second" = "word")) %>% #went from 236430 to 46028
  filter(str_detect(first, "[a-z]") &
         str_detect(second, "[a-z]")) %>% #goes to 43675
  filter(!str_detect(bigram, "t.co"))  #Reduced bigrams to 43490

#Count how many times each word appears in all coverage
words_count <- join_words %>%
  group_by(word) %>%
  count() %>%
  arrange(-n) %>%
  print

#Count how many times each word pair appears in all coverage
#28726 unique word pairs
bigrams_count <- join_bigrams %>%
  group_by(bigram) %>%
  summarize (count = n(),
          stories = n_distinct(url)) %>%
  arrange(-count) %>%
  #Manually remove frequent word pairs that are inherent to NPR (reporter names, npr processes, etc)
  filter (!bigram %in% c("hide caption", "desk climate", "digital stub", "ramirez franco", "atc digital", "digital stories", "tbd digital", "climate desk", "digital story", "lauren sommer", "stories coming", "michael copley", "margaret cirino", "nate rott", "npr hide", "getty images", "images hide", "ap hide", "npr news", "ryan kellman", "kellman npr", "apple podcasts", "npr's climate")) %>%
  print 
```

# Visualization
## Total word frequency
This code displays the top 25 word pairs by the total count of appearances across all stories published in this six-month period.
```{r}
q <- bigrams_count %>%
  head(25)
  
graph <- ggplot(data=q, aes(x=fct_reorder(bigram, count), y=count)) +
  geom_bar(stat="identity") + 
  coord_flip() + 
  geom_text(aes(label = stories), vjust = 0.45, hjust=1.2, colour = "gold", size = 3) +
  labs(title="Top 25 word pairs from NPR's climate coverage (Oct. 2022-April 2023)", 
       x="Word Pairs", 
       y="Number of times mentioned in NPR's climate coverage",
       subtitle="Yellow figures show count of stories mentioning topic") +
  scale_y_continuous(breaks=seq(0,600,by=100),
                     minor_breaks = seq(0,600,by=25)) +
  theme_linedraw() +
  theme (plot.subtitle = element_text(face = "italic", size=9))

print(graph)
ggsave(plot = graph,
       file = "outputs/Word pairs1.png")  
```

## Story frequency
The phrase "responsible solar" was included 37 times in just one story published Feb. 18, 2023, and the phrase "heat pumps" was mentioned three dozen times in two stories at the end of March, so these are not truly frequently covered topics. 

Instead, this code charts the number of unique articles in which each topic appeared on NPR.org.
```{r}
q2 <- bigrams_count %>%
  arrange (-stories) %>%
  head(25)

graph2 <- ggplot(data=q2, aes(x=fct_reorder(bigram, stories), y=stories)) +
  geom_bar(stat="identity") + 
  coord_flip() + 
  #geom_text(aes(label = stories), vjust = 0.45, hjust=1.2, colour = "yellow", size = 3) +
  labs(title="Top 25 word pairs from NPR's climate coverage (Oct. 2022-April 2023)", 
       x="Word Pairs", 
       y="Number of stories mentioning term in NPR's climate coverage",
       subtitle="Evaluated from 272 articles pulled from npr.org/sections/climate/archive"
       ) +
  scale_y_continuous(breaks=seq(0,175,by=20)) +
  theme_linedraw() +
  theme (plot.subtitle = element_text(face = "italic", size=9))

print(graph2)  
ggsave(plot = graph2,
       file = "outputs/Word pairs2.png")  
```

## "Health"
<b>Question: How many stories mentioned "health"?</b> <br>
<b>Answer</b>: Only two stories mention "mental health" or "health concerns."

```{r}
topic1 <- bigrams_count %>%
  filter (str_detect (bigram, "health")) %>%
  slice_max (n=25, stories) %>%
  head (25) %>%
  mutate (bigram=fct_reorder(bigram, stories))

topic1$color <- ifelse(topic1$bigram %in% c("mental health", "human health"), "red", "black")
topic1$color <- as.factor(topic1$color)

colors <- topic1$color[order(topic1$stories, topic1$bigram)]

topic1 <- topic1 %>%
  ggplot(aes(x=bigram, y=stories, fill=color)) +
  geom_bar(stat="identity") + 
  coord_flip() + 
  scale_y_continuous(breaks=seq(1,13,by=1)) +
  labs(title="\"Health\" within NPR's climate coverage (Oct. 2022-April 2023)", x="Word Pairs", y="Count of stories") +
  theme_linedraw() +
  scale_fill_manual(values = c("#888888", "#CC3D3D")) +
  #scale_fill_manual(values = c("mental health" = "#", "human health" = "#CC3D3D")) +
  theme(legend.position = "none") +
  theme(axis.text.y = element_text(color = colors))

print(topic1)  
ggsave(plot = topic1,
       file = "outputs/Word pairs-health.png")
```

Which stories reference the most frequent health topic: "public health"?
```{r}
search_term <- bigrams_count %>%
  filter (str_detect (bigram, "health")) %>%
  slice_max (n=1, stories) %>%
  pull (bigram)
  
join_bigrams %>%
  filter (str_detect (bigram, search_term)) %>%
  select (bigram, date, headline, url) %>%
  distinct (bigram, date, headline, url) %>%
  print
```

## "Native American"
<b>Question: How many stories mentioned "Native", "indigenous", or "tribal"?</b> <br>
<b>Answer</b>: A half-dozen mention "indigenous people."
```{r}
topic2 <- bigrams_count %>%
  filter (str_detect (bigram, "\\bnative|indigenous|tribal\\b")) %>%
  filter (!bigram %in% c("native species", "native plants", "gallon native")) %>%
  slice_max (n=25, stories) %>%
  head (25) %>%
  mutate (bigram=fct_reorder(bigram, stories))

topic2$color <- ifelse(topic2$bigram %in% c("displace native", "displace indigenous"), "red", "black")
topic2$color <- as.factor(topic2$color)

colors <- topic2$color[order(topic2$stories, topic2$bigram)]

topic2 <- topic2 %>%
  ggplot(aes(x=bigram, y=stories, fill=color)) +
  geom_bar(stat="identity") + 
  coord_flip() + 
  scale_y_continuous(breaks=seq(1,13,by=1)) +
  labs(title="Native American, indigenous, or tribal", x="Word Pairs", y="Count of stories in NPR's climate coverage (Oct. 2022-April 2023)") +
  theme_linedraw() +
  scale_fill_manual(values = c("#888888", "#CC3D3D")) +
  theme(legend.position = "none") +
  theme(axis.text.y = element_text(color = colors))

print(topic2)  
ggsave(plot = topic2,
       file = "outputs/Word pairs-native.png")  
```

Which stories reference the most frequent topic connected to Native Americans, indigenous or tribal issues?
```{r}
search_term <- bigrams_count %>%
  filter (str_detect (bigram, "\\bnative|indigenous|tribal\\b")) %>%
  slice_max (n=1, stories) %>%
  pull (bigram)
  
join_bigrams %>%
  filter (bigram==search_term) %>%
  select (bigram, date, headline, url) %>%
  distinct (bigram, date, headline, url) %>%
  print
```

## "Disadvantaged communities"
<b>Question: How many stories mentioned poverty, people of color or various "disadvantaged communities"?</b> <br>
<b>Answer</b>: Low-income is mentioned in 18 stories, but most other terms are mentioned in just one story each.
```{r}
list <- c("disadvantaged", "low income", "low-income", "poverty")
          #"african american", "hispanic", "latino", "asian american", "minority", "\\bblack\\b"
          #"disability", "rural", "\\byouth\\b", "lgbt", "women", "\\btrans\\b", "\\btransit\\b")

topic4 <- bigrams_count %>%
  filter (str_detect (bigram, paste(list, collapse="|"))) %>%
  slice_max (n=25, stories) %>%
  #Remove false flags
  filter (!(bigram %in% c("black browed", "black paint", "threw black", "black baseball", "black granite", "black hawk", "black lung", "black mesh", "black objects", "black robe", "black scar", "black tea", "city black", "pull black", "black bicycle"))) %>%
  head (25) %>%
  ggplot(aes(x=fct_reorder(bigram, stories), y=stories)) +
  geom_bar(stat="identity") + 
  coord_flip() + 
  scale_y_continuous(breaks=seq(1,20,by=1)) +
  labs(title="Disadvantaged communities & poverty", x="Word Pairs", y="Count of stories in NPR's climate coverage (Oct. 2022-April 2023)") +
  theme_linedraw() +
  scale_fill_manual(values = c("#888888"))

print(topic4)  
ggsave(plot = topic4,
       file = "outputs/Word pairs-poverty.png")  
```

Which stories reference poverty, people of color or various "disadvantaged communities"??
```{r}
join_bigrams %>%
  filter (str_detect (bigram, paste(list, collapse="|"))) %>%
  select (bigram, date, headline, url) %>%
  distinct (date, headline, url) %>%
  print
```

## "Sewer overflows"
<a name="sewer"></a>
<b>Question: How many stories mentioned sewer overflows and the health problems they cause?</b><br>
<b>Answer</b>: Seven stories, largely dealing with the rain that's inundated California this winter.
```{r}
list <- c("sewer", "sewage", "overflow")

topic5 <- bigrams_count %>%
  filter (str_detect (bigram, paste(list, collapse="|"))) %>%
  slice_max (n=25, stories) %>%
  #Remove false flags
#  filter (!(bigram %in% c("black browed", "black paint"))) %>%
  head (25) %>%
  ggplot(aes(x=fct_reorder(bigram, stories), y=stories)) +
  geom_bar(stat="identity") + 
  coord_flip() + 
  scale_y_continuous(breaks=seq(1,20,by=1)) +
  labs(title="Sewers, sewage, overflow", x="Word Pairs", y="Count of stories in NPR's climate coverage (Oct. 2022-April 2023)") +
  theme_linedraw() 

print(topic5)  
ggsave(plot = topic5,
       file = "outputs/Word pairs-sewer.png")  
```
Which stories reference sewer overflows?
```{r}
join_bigrams %>%
  filter (str_detect (bigram, paste(list, collapse="|"))) %>%
  select (bigram, date, headline, url) %>%
  distinct (date, headline, url) %>%
  print
```

## "Flaring"
<a name="flaring"></a>
<b>Question: How many stories mentioned "flare" or "flaring"?</b> <br>
<b>Answer</b>: Very few stories. Just three stories had any mention of these terms, and one was AP copy.
```{r}
#Very few stories mention flaring
topic3 <- bigrams_count %>%
  filter (str_detect (bigram, "flare|flaring")) %>%
  filter (!bigram %in% c("flare melilla", "eyes flare")) %>%
  slice_max (n=25, stories) %>%
  head (25) %>%
  ggplot(aes(x=fct_reorder(bigram, stories,), y=stories)) +
  geom_bar(stat="identity") + 
  coord_flip() + 
  scale_y_continuous(breaks=seq(1,13,by=1)) +
  labs(title="\"Flaring\" within NPR's climate coverage (Oct. 2022-April 2023)", x="Word Pairs", y="Count of stories") +
  theme_linedraw() 

print(topic3)  
ggsave(plot = topic3,
       file = "outputs/Word pairs-flaring.png")  
```

Which stories reference flaring?
```{r}
join_bigrams %>%
  filter (str_detect (bigram, "flare|flaring")) %>%
  filter (!bigram %in% c("flare melilla", "eyes flare")) %>%
  select (bigram, date, headline, url) %>%
  distinct (bigram, date, headline, url) %>%
  print
```

This code creates a data frame showing the count of stories mentioning each word pair in each month, to show change over time.
```{r eval=FALSE}
#How many stories were these terms mentioned in?
search_terms <- bigrams_count %>%
  #filter (n>9) %>%
  pull (bigram)

#search_terms <- c(q$bigram, "public health", "health risks")

analysis <- tibble()
for (i in 1:length(search_terms)){
  #i <- 1
  search <- search_terms[i]
  step_df <- join_bigrams %>%
    filter (bigram==search) %>%
    distinct (headline, date, url) %>%
    select (headline, date, url) %>%
    mutate(time_floor = floor_date(date, unit = "1 month")) %>%
    count(time_floor) %>%
    adorn_totals() %>%
    mutate (bigram=search) 
  
  analysis <- bind_rows (analysis, step_df)
}

analysis2 <- analysis %>%
  #filter(row_number()<60483) %>%
  pivot_wider (names_from = "time_floor",
               values_from = "n",
               values_fill = 0)

write_csv (analysis2, "bigrams_story_count.csv")
```

## Chart story frequency over time

```{r message=FALSE}
analysis2 <- read_csv( "bigrams_story_count.csv")

#Double check story count performed above
#analysis2 %>%
#  slice_max(n=25, Total) %>%
#  ggplot(aes(x=fct_reorder(bigram, Total), y=Total)) +
#  geom_bar(stat="identity") + coord_flip() + 
#  labs(title="Top word pairs in NPR's climate coverage (Oct. 2022-April 2023)", x="Word Pairs", y="Count of stories") %>%
#  print

search_terms <- analysis2 %>%
  slice_max(Total, n=5) %>%
  pull (bigram)

search_terms2 <- c(search_terms, "low income", "indigenous people", "mental health", "human health", "public health") 

#Chart top 5 topics' frequency over time
graph4 <- analysis2 %>%
  filter (bigram %in% search_terms2) %>%
  select (-c(Total, `2023-04-01`)) %>%
  pivot_longer (cols=(2:7),
                names_to="date", 
                values_to="value") %>%
  mutate (date = ymd(date),
          cat = if_else (bigram %in% search_terms, "Most frequent topics", "Selected health topics")) %>%
  ggplot(aes(x=date, y=value, group=bigram, color=bigram)) +
  geom_line() +
  geom_line(size = 1.3) +
  facet_wrap(~cat) + 
  labs(title="Topic frequency within NPR's climate coverage", 
       x=NULL, 
       y="Count of stories",
       color="Topic") +
  theme_linedraw() +
  scale_x_date(date_breaks = "months" , date_labels = "%b") + 
  scale_y_continuous(breaks = seq(0,55, by=5)) 

print(graph4)  
ggsave(plot = graph4,
       file = "outputs/Word pairs-over time.png")  
```

